{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripts to tune my models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "\n",
    "class SimpleUNet(tf.keras.Model):\n",
    "    def __init__(self, num_filters=32, depth=4):\n",
    "        super(SimpleUNet, self).__init__()\n",
    "        self.num_filters = num_filters\n",
    "        self.depth = depth\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_blocks = [self.encoder_block(num_filters * 2**i) for i in range(depth)]\n",
    "        self.flatten_layer = tf.keras.layers.Flatten()  # Add Flatten layer\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck_conv = tf.keras.layers.Conv2D(num_filters * 2**depth, 3, activation='relu', padding='same')\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_blocks = [self.decoder_block(num_filters * 2**(depth - i - 1)) for i in range(depth)]\n",
    "\n",
    "        # Output convolution\n",
    "        self.output_conv = tf.keras.layers.Conv2D(1, 1, activation='sigmoid', padding='same')\n",
    "\n",
    "    def encoder_block(self, filters):\n",
    "        block = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2D(filters, 3, activation='relu', padding='same'),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        ])\n",
    "        return block\n",
    "\n",
    "    def decoder_block(self, filters):\n",
    "        block = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv2DTranspose(filters, 2, strides=(2, 2), padding='same'),\n",
    "            tf.keras.layers.Conv2D(filters, 3, activation='relu', padding='same')\n",
    "        ])\n",
    "        return block\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs\n",
    "        skip_connections = []\n",
    "\n",
    "        # Encoder\n",
    "        for block in self.encoder_blocks:\n",
    "            x = block(x)\n",
    "            skip_connections.append(x)\n",
    "\n",
    "        x = self.flatten_layer(x) \n",
    "        # Bottleneck\n",
    "        x = self.bottleneck_conv(x)\n",
    "\n",
    "        # Decoder\n",
    "        for i, block in enumerate(self.decoder_blocks):\n",
    "            x = block(x)\n",
    "            x = tf.keras.layers.Concatenate()([x, skip_connections[-i - 1]])\n",
    "\n",
    "        # Output convolution\n",
    "        x = self.output_conv(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# custom loss functions\n",
    "def psnr_loss(y_true, y_pred):\n",
    "    return -tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "param_grid = {\n",
    "    'num_filters': [32, 64],\n",
    "    'depth': [3, 4],\n",
    "    'learning_rate': [0.01, 0.001],\n",
    "    'optimizer': [Adam, RMSprop]\n",
    "}\n",
    "input_folder = '/jupyter/work/fyp/data/img_rec/4th_set_noise/4_noise'\n",
    "output_folder = '/jupyter/work/fyp/data/img_rec/4th_set_noise/16_noise'\n",
    "\n",
    "\n",
    "def load_and_preprocess_sinograms(input_folder, output_folder):\n",
    "    input_images = []\n",
    "    output_images = []\n",
    "\n",
    "    input_filenames = sorted(os.listdir(input_folder))\n",
    "    output_filenames = sorted(os.listdir(output_folder))\n",
    "\n",
    "    for input_filename, output_filename in zip(input_filenames, output_filenames):\n",
    "        input_img_path = os.path.join(input_folder, input_filename)\n",
    "        output_img_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "        input_img = cv2.imread(input_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        output_img = cv2.imread(output_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if input_img is not None and output_img is not None:\n",
    "            input_img = cv2.resize(input_img, (128, 128))  \n",
    "            input_img = input_img.astype('float32') / 255.0  \n",
    "            input_images.append(input_img)\n",
    "\n",
    "            output_img = cv2.resize(output_img, (128, 128))  \n",
    "            output_img = output_img.astype('float32') / 255.0  \n",
    "            output_images.append(output_img)\n",
    "        else:\n",
    "            print(f\"Error loading images: {input_img_path}, {output_img_path}\")\n",
    "\n",
    "    if not input_images or not output_images:\n",
    "        raise ValueError(\"No valid images found in the input or output folder\")\n",
    "\n",
    "    print(\"Input images shape:\", input_images[0].shape)\n",
    "    print(\"Output images shape:\", output_images[0].shape)\n",
    "\n",
    "    # split data into train and test sets\n",
    "    split_ratio = 0.8  # 80% train, 20% test\n",
    "    split_index = int(len(input_images) * split_ratio)\n",
    "\n",
    "    train_input_images, test_input_images = input_images[:split_index], input_images[split_index:]\n",
    "    train_output_images, test_output_images = output_images[:split_index], output_images[split_index:]\n",
    "\n",
    "    return train_input_images, train_output_images, test_input_images, test_output_images\n",
    "\n",
    "\n",
    "\n",
    "def train_evaluate_model(params, train_input_images, train_output_images, test_input_images, test_output_images):\n",
    "    unet_model = SimpleUNet(num_filters=params['num_filters'], depth=params['depth'])\n",
    "    optimizer = params['optimizer'](learning_rate=params['learning_rate'])\n",
    "    unet_model.compile(optimizer=optimizer, loss=psnr_loss, metrics=['accuracy'])\n",
    "\n",
    "    history = unet_model.fit(train_input_images, train_output_images, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "    loss, accuracy = unet_model.evaluate(test_input_images, test_output_images)\n",
    "\n",
    "    return loss, accuracy, history.history['loss'], history.history['val_loss']\n",
    "\n",
    "# grid search\n",
    "def grid_search(param_grid, train_input_images, train_output_images, test_input_images, test_output_images):\n",
    "    best_model = None\n",
    "    best_params = None\n",
    "    best_loss = float('inf')\n",
    "    losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        print(\"Training model with parameters:\", params)\n",
    "\n",
    "        loss, _, history_loss, history_val_loss = train_evaluate_model(params, train_input_images, train_output_images, test_input_images, test_output_images)\n",
    "        losses.append(history_loss)\n",
    "        val_losses.append(history_val_loss)\n",
    "\n",
    "        if loss < best_loss:\n",
    "            best_model = params\n",
    "            best_loss = loss\n",
    "\n",
    "    return best_model, losses, val_losses\n",
    "\n",
    "train_input_images, train_output_images, test_input_images, test_output_images = load_and_preprocess_sinograms(input_folder, output_folder)\n",
    "\n",
    "# Perform grid search\n",
    "best_params, losses, val_losses = grid_search(param_grid, train_input_images, train_output_images, test_input_images, test_output_images)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure(figsize=(10, 6))\n",
    "for i in range(len(losses)):\n",
    "    plt.plot(losses[i], label=f'Model {i+1} Train')\n",
    "    plt.plot(val_losses[i], label=f'Model {i+1} Validation')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss for Different Model Configurations')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Width and depth tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define U-Net model architecture\n",
    "def build_unet_model(depth, width, output_size=(128, 128)):\n",
    "    inputs = tf.keras.Input(shape=(128, 128, 1))\n",
    "    x = inputs\n",
    "    # Encoder\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2D(width, 3, activation='relu', padding='same')(x)\n",
    "        x = layers.Conv2D(width, 3, activation='relu', padding='same')(x)\n",
    "        if x.shape[1] >= 2 and x.shape[2] >= 2:\n",
    "            x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        else:\n",
    "            break  # Skip pooling if input size is too small\n",
    "    # Bottleneck\n",
    "    x = layers.Conv2D(width, 3, activation='relu', padding='same')(x)\n",
    "    # Decoder\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2DTranspose(width, 2, strides=(2, 2), padding='same')(x)\n",
    "        x = layers.Conv2D(width, 3, activation='relu', padding='same')(x)\n",
    "    # Resize output to desired shape\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
    "    outputs = tf.image.resize(outputs, output_size)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"sinogram_unet_{width}_{depth}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# Define fitness function\n",
    "def calculate_fitness(model, test_input_images, test_output_images):\n",
    "    # Evaluate the model\n",
    "    test_loss = model.evaluate(test_input_images, test_output_images, verbose=0)\n",
    "    # Calculate Dice coefficient, F1 score, PSNR\n",
    "    y_pred = model.predict(test_input_images)\n",
    "    dice = dice_coefficient(test_output_images, y_pred)\n",
    "    f1 = f1_score(test_output_images, y_pred)\n",
    "    psnr_val = psnr(test_output_images, y_pred)\n",
    "    # Combine metrics into a single fitness value (maximize each metric)\n",
    "    fitness = dice + f1 + psnr_val\n",
    "    return fitness\n",
    "\n",
    "\n",
    "# Load and preprocess images (Replace with your actual data loading code)\n",
    "train_input_images = np.random.rand(100, 128, 128, 1).astype('float32')\n",
    "train_output_images = np.random.rand(100, 128, 128, 1).astype('float32')\n",
    "test_input_images = np.random.rand(20, 128, 128, 1).astype('float32')\n",
    "test_output_images = np.random.rand(20, 128, 128, 1).astype('float32')\n",
    "\n",
    "# Define parameters for tuning\n",
    "depth_values = [3, 4, 5, 8]\n",
    "width_values = [16, 32, 64, 128]\n",
    "\n",
    "# Placeholder for storing results\n",
    "results = []\n",
    "\n",
    "# Loop over depth and width combinations\n",
    "for depth in depth_values:\n",
    "    for width in width_values:\n",
    "        # Build model\n",
    "        model = build_unet_model(depth, width)\n",
    "        model.compile(optimizer='adam', loss='mse') s\n",
    "        history = model.fit(train_input_images, train_output_images, epochs=200, batch_size=32, verbose=0)\n",
    "        # Calculate fitness\n",
    "        fitness = calculate_fitness(model, test_input_images, test_output_images)\n",
    "        results.append((depth, width, fitness))\n",
    "\n",
    "fig, axes = plt.subplots(len(depth_values), len(width_values), figsize=(15, 10))\n",
    "for i, depth in enumerate(depth_values):\n",
    "    for j, width in enumerate(width_values):\n",
    "        ax = axes[i, j]\n",
    "        ax.plot(range(10), np.random.rand(10))  # Placeholder data for plotting\n",
    "        ax.set_title(f\"Depth={depth}, Width={width}, Fitness={results[i*len(width_values)+j][2]:.4f}\")\n",
    "        ax.set_xlabel(\"Epochs\")\n",
    "        ax.set_ylabel(\"Fitness\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structural and algorithmic hyperparameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "\n",
    "# U-Net model architecture\n",
    "def build_unet_model(depth, width, activation, learning_rate, optimizer, output_size=(128, 128)):\n",
    "    inputs = tf.keras.Input(shape=(128, 128, 1))\n",
    "    x = inputs\n",
    "    # Encoder\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2D(width, 3, activation=activation, padding='same')(x)\n",
    "        x = layers.Conv2D(width, 3, activation=activation, padding='same')(x)\n",
    "        if x.shape[1] >= 2 and x.shape[2] >= 2:\n",
    "            x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        else:\n",
    "            break  # Skip pooling if input size is too small\n",
    "    # Bottleneck\n",
    "    x = layers.Conv2D(width, 3, activation=activation, padding='same')(x)\n",
    "    # Decoder\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2DTranspose(width, 2, strides=(2, 2), padding='same')(x)\n",
    "        x = layers.Conv2D(width, 3, activation=activation, padding='same')(x)\n",
    "    outputs = layers.Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
    "    outputs = tf.image.resize(outputs, output_size)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"sinogram_unet_{width}_{depth}\")\n",
    "    \n",
    "    # Compile model with provided optimizer and learning rate\n",
    "    model.compile(optimizer=optimizer(learning_rate=learning_rate), loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# fitness function\n",
    "def calculate_fitness(model, test_input_images, test_output_images):\n",
    "    # Evaluate the model\n",
    "    test_loss = model.evaluate(test_input_images, test_output_images, verbose=0)\n",
    "    # Calculate Dice coefficient, F1 score, PSNR\n",
    "    y_pred = model.predict(test_input_images)\n",
    "    dice = dice_coefficient(test_output_images, y_pred)\n",
    "    f1 = f1_score(test_output_images, y_pred)\n",
    "    psnr_val = psnr(test_output_images, y_pred)\n",
    "    fitness = dice + f1 + psnr_val\n",
    "    return fitness\n",
    "\n",
    "train_input_images = np.random.rand(100, 128, 128, 1).astype('float32')\n",
    "train_output_images = np.random.rand(100, 128, 128, 1).astype('float32')\n",
    "test_input_images = np.random.rand(20, 128, 128, 1).astype('float32')\n",
    "test_output_images = np.random.rand(20, 128, 128, 1).astype('float32')\n",
    "\n",
    "# Define parameters for tuning\n",
    "depth_values = [3, 4, 5, 8]\n",
    "width_values = [16, 32, 64, 128]\n",
    "activation_functions = ['relu', 'elu', 'selu']\n",
    "learning_rates = [1e-2, 1e-3, 1e-4]\n",
    "optimizers = [tf.keras.optimizers.Adam, tf.keras.optimizers.RMSprop, tf.keras.optimizers.SGD]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop over hyperparameter combinations\n",
    "for depth, width, activation, learning_rate, optimizer in product(depth_values, width_values, activation_functions, learning_rates, optimizers):\n",
    "    # Build model\n",
    "    model = build_unet_model(depth, width, activation, learning_rate, optimizer)\n",
    "    # Train model (Replace with your actual training code)\n",
    "    history = model.fit(train_input_images, train_output_images, epochs=200, batch_size=32, verbose=0)\n",
    "    # Calculate fitness\n",
    "    fitness = calculate_fitness(model, test_input_images, test_output_images)\n",
    "    results.append((depth, width, activation, learning_rate, optimizer, fitness))\n",
    "\n",
    "# Print results\n",
    "for result in results:\n",
    "    print(f\"Depth={result[0]}, Width={result[1]}, Activation={result[2]}, Learning Rate={result[3]}, Optimizer={result[4]}, Fitness={result[5]:.4f}\")\n",
    "    \n",
    "fig, axes = plt.subplots(len(depth_values), len(width_values), figsize=(15, 10))\n",
    "for i, depth in enumerate(depth_values):\n",
    "    for j, width in enumerate(width_values):\n",
    "        ax = axes[i, j]\n",
    "        fitness_values = []\n",
    "        for result in results:\n",
    "            if result[0] == depth and result[1] == width:\n",
    "                fitness_values.append(result[5])\n",
    "        ax.bar(range(len(fitness_values)), fitness_values, color='blue')\n",
    "        ax.set_title(f\"Depth={depth}, Width={width}\")\n",
    "        ax.set_xlabel(\"Hyperparameter Combination\")\n",
    "        ax.set_ylabel(\"Fitness\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a30072751ebdd9e38b3a3723274826dbc6c580cc83e8305b93d67478e6165946"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
