{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tuning for sinogram unet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.image import ssim\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# loss functions\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    return 1 - tf.reduce_mean(ssim(y_true, y_pred, max_val=1.0))\n",
    "\n",
    "# PSNR loss function\n",
    "def psnr_loss(y_true, y_pred):\n",
    "    return -tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "# load and preprocess sinogram images\n",
    "def load_and_preprocess_sinograms(input_folder, output_folder):\n",
    "    input_images = []\n",
    "    output_images = []\n",
    "    \n",
    "    input_filenames = sorted(os.listdir(input_folder))\n",
    "    output_filenames = sorted(os.listdir(output_folder))\n",
    "    \n",
    "    for input_filename, output_filename in zip(input_filenames, output_filenames):\n",
    "        input_img_path = os.path.join(input_folder, input_filename)\n",
    "        output_img_path = os.path.join(output_folder, output_filename)\n",
    "        \n",
    "        input_img = cv2.imread(input_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        output_img = cv2.imread(output_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if input_img is not None and output_img is not None:\n",
    "            input_img = cv2.resize(input_img, (128, 128)) \n",
    "            output_img = cv2.resize(output_img, (128, 128))  \n",
    "            \n",
    "            input_img = input_img.astype('float32') / 255.0  \n",
    "            output_img = output_img.astype('float32') / 255.0  \n",
    "            \n",
    "            input_images.append(input_img)\n",
    "            output_images.append(output_img)\n",
    "    \n",
    "    return np.array(input_images), np.array(output_images)\n",
    "\n",
    "input_folder = '/jupyter/work/fyp/data/sinograms/1st_set/lsd'\n",
    "output_folder = '/jupyter/work/fyp/data/sinograms/1st_set/hsd'\n",
    "\n",
    "input_images, output_images = load_and_preprocess_sinograms(input_folder, output_folder)\n",
    "\n",
    "train_input_images, test_input_images, train_output_images, test_output_images = train_test_split(\n",
    "    input_images, output_images, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# best width and depth values\n",
    "best_width = 128\n",
    "best_depth = 5\n",
    "\n",
    "# define hyperparameters to tune\n",
    "learning_rates = [0.001, 0.0001]\n",
    "optimizers = [Adam, tf.keras.optimizers.SGD]\n",
    "loss_functions = [psnr_loss, ssim_loss]\n",
    "\n",
    "# collect loss values for different hyperparameter combinations\n",
    "loss_combinations = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for optimizer in optimizers:\n",
    "        for loss_func in loss_functions:\n",
    "            # U-Net model architecture for sinograms\n",
    "            inputs = tf.keras.Input(shape=(128, 128, 1))\n",
    "            x = inputs\n",
    "\n",
    "            # Encoder\n",
    "            for _ in range(best_depth):\n",
    "                x = layers.Conv2D(best_width, 3, activation='relu', padding='same')(x)\n",
    "                x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "            # Bottleneck\n",
    "            x = layers.Conv2D(best_width, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "            # Decoder\n",
    "            for _ in range(best_depth):\n",
    "                x = layers.Conv2DTranspose(best_width, 2, strides=(2, 2), padding='same')(x)\n",
    "                x = layers.Conv2D(best_width, 3, activation='relu', padding='same')(x)\n",
    "\n",
    "            outputs = layers.Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "            model = tf.keras.Model(inputs=inputs, outputs=outputs, name=f\"sinogram_unet_{best_width}_{best_depth}\")\n",
    "\n",
    "            optimizer_instance = optimizer(learning_rate=lr)\n",
    "            model.compile(optimizer=optimizer_instance, loss=loss_func, metrics=['accuracy'])\n",
    "\n",
    "            # train the model\n",
    "            print(f\"Training model with LR={lr}, Optimizer={optimizer.__name__}, Loss={loss_func.__name__}\")\n",
    "            history = model.fit(train_input_images, train_output_images, epochs=50, batch_size=32, validation_split=0.2, verbose=1)\n",
    "\n",
    "            # evaluate the model\n",
    "            test_loss = model.evaluate(test_input_images, test_output_images, verbose=1)\n",
    "\n",
    "            # collect test loss values\n",
    "            loss_combinations.append((lr, optimizer.__name__, loss_func.__name__, test_loss[0]))\n",
    "\n",
    "            print(f\"Test Loss: {test_loss[0]}\")\n",
    "\n",
    "for lr, optimizer, loss_func, test_loss in loss_combinations:\n",
    "    print(f\"LR={lr}, Optimizer={optimizer}, Loss={loss_func}, Test Loss={test_loss}\")\n",
    "\n",
    "# plot the loss for each combination\n",
    "loss_values = [loss for _, _, _, loss in loss_combinations]\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Combination')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss for Different Hyperparameter Combinations')\n",
    "plt.xticks(range(len(loss_combinations)), [f\"{lr}, {optimizer}, {loss_func}\" for lr, optimizer, loss_func, _ in loss_combinations], rotation=90)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
