{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNet for perfect sinogram data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "from PIL import Image\n",
    "\n",
    "# UNet model\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.encoder1 = DoubleConv(in_channels, 64)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = DoubleConv(64, 128)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder3 = DoubleConv(128, 256)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder4 = DoubleConv(256, 512)\n",
    "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bottleneck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder1 = DoubleConv(1024, 512)\n",
    "        self.upconv2 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder2 = DoubleConv(512, 256)\n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = DoubleConv(256, 128)\n",
    "        self.upconv4 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder4 = DoubleConv(128, 64)\n",
    "\n",
    "        self.output = nn.Conv2d(64, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "        enc3 = self.encoder3(self.pool2(enc2))\n",
    "        enc4 = self.encoder4(self.pool3(enc3))\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = self.bottleneck(self.pool4(enc4))\n",
    "\n",
    "        # Decoder\n",
    "        dec1 = self.upconv1(bottleneck)\n",
    "        dec1 = self.decoder1(torch.cat([enc4, dec1], dim=1))\n",
    "        dec2 = self.upconv2(dec1)\n",
    "        dec2 = self.decoder2(torch.cat([enc3, dec2], dim=1))\n",
    "        dec3 = self.upconv3(dec2)\n",
    "        dec3 = self.decoder3(torch.cat([enc2, dec3], dim=1))\n",
    "        dec4 = self.upconv4(dec3)\n",
    "        dec4 = self.decoder4(torch.cat([enc1, dec4], dim=1))\n",
    "\n",
    "        # Output\n",
    "        output = self.output(dec4)\n",
    "        return output\n",
    "\n",
    "# dataset class\n",
    "class SinogramDataset(Dataset):\n",
    "    def __init__(self, input_folder, output_folder, transform=None):\n",
    "        self.input_folder = input_folder\n",
    "        self.output_folder = output_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        self.input_images = sorted(os.listdir(input_folder))\n",
    "        self.output_images = sorted(os.listdir(output_folder))\n",
    "\n",
    "        # Ensure that the input and output images are paired correctly based on order\n",
    "        assert len(self.input_images) == len(self.output_images), \"Mismatch in the number of input and output images\"\n",
    "        self.num_images = len(self.input_images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_image_path = os.path.join(self.input_folder, self.input_images[index])\n",
    "        output_image_path = os.path.join(self.output_folder, self.output_images[index])\n",
    "\n",
    "        input_image = Image.open(input_image_path).convert('RGB')\n",
    "        output_image = Image.open(output_image_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            input_image = self.transform(input_image)\n",
    "            output_image = self.transform(output_image)\n",
    "\n",
    "        return input_image, output_image\n",
    "\n",
    "\n",
    "# data transformations\n",
    "data_transform = Compose([Resize((128, 128)), ToTensor()])\n",
    "\n",
    "input_folder = '/home/hinata/code/fyp/images/ml_images/rec/lsd'\n",
    "output_folder = '/home/hinata/code/fyp/images/ml_images/rec/hsd'\n",
    "dataset = SinogramDataset(input_folder, output_folder, transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "#UNet model, loss function, and optimizer\n",
    "model = UNet(in_channels=3, out_channels=3)\n",
    "\n",
    "# # Move the model to TPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if str(device) == \"cuda\":\n",
    "#     model = model.cuda()\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.00001)\n",
    "\n",
    "# # Move the model to GPU if available\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "# # Move the criterion to GPU if available (optional)\n",
    "# criterion = nn.MSELoss().to(device)\n",
    "\n",
    "num_epochs = 250\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss}\")\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'sngrm_to_sngrm_model.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing my unet on new sinograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from skimage.filters import gaussian\n",
    "from torchvision.transforms import ToTensor, Resize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def list_images(folder_path):\n",
    "    return [os.path.join(folder_path, filename) for filename in os.listdir(folder_path) if filename.endswith('.png')]\n",
    "\n",
    "input_sinogram_folder = '/content/drive/MyDrive/Colab Notebooks/fyp/ml_images/sinogram_images/4a'\n",
    "ground_truth_sinogram_folder = '/content/drive/MyDrive/Colab Notebooks/fyp/ml_images/sinogram_images/16a'\n",
    "\n",
    "input_sinogram_paths = list_images(input_sinogram_folder)\n",
    "\n",
    "ground_truth_sinogram_paths = list_images(ground_truth_sinogram_folder)\n",
    "\n",
    "num_images_to_select = 20  # Change this value as needed\n",
    "\n",
    "# Randomly select a specific number of images\n",
    "random_input_sinogram_paths = random.sample(input_sinogram_paths, num_images_to_select)\n",
    "random_ground_truth_sinogram_paths = random.sample(ground_truth_sinogram_paths, num_images_to_select)\n",
    "\n",
    "model = UNet(in_channels=3, out_channels=3)  # Assuming UNet is defined in the same module\n",
    "\n",
    "state_dict = torch.load('sinogram_enhancement_model.pth', map_location=torch.device('cpu'))\n",
    "\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# store PSNR and SSIM values\n",
    "psnr_values = []\n",
    "ssim_values = []\n",
    "\n",
    "# resize transform\n",
    "resize_transform = Resize((128, 128))\n",
    "\n",
    "for input_path, gt_path in zip(input_sinogram_paths, ground_truth_sinogram_paths):\n",
    "    input_sinogram = Image.open(input_path).convert('L') \n",
    "    input_sinogram = resize_transform(input_sinogram)\n",
    "    input_sinogram = ToTensor()(input_sinogram)  \n",
    "    input_sinogram = torch.cat([input_sinogram] * 3, dim=0)  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_sinogram = model(input_sinogram.unsqueeze(0))\n",
    "\n",
    "    # denoise the output sinogram\n",
    "    denoised_output = gaussian(output_sinogram.numpy()[0, 0], sigma=1)\n",
    "\n",
    "    # convert tensors to numpy arrays\n",
    "    denoised_output_np = denoised_output\n",
    "    input_sinogram_np = input_sinogram[0].numpy()\n",
    "\n",
    "    # save denoised output as image\n",
    "    output_dir = '/content/drive/MyDrive/Colab Notebooks/fyp/img/output/3'\n",
    "    output_filename = os.path.join(output_dir, f\"predicted_sinogram_{os.path.basename(input_path)}\")\n",
    "    plt.imsave(output_filename, denoised_output_np, cmap='gray')\n",
    "\n",
    "    # PSNR\n",
    "    mse_loss = F.mse_loss(torch.from_numpy(np.expand_dims(denoised_output_np, axis=0)), input_sinogram.unsqueeze(0))\n",
    "    psnr = 20 * torch.log10(1.0 / torch.sqrt(mse_loss))\n",
    "    psnr_values.append(psnr.item())\n",
    "\n",
    "    # SSIM\n",
    "    ssim_value, _ = ssim(denoised_output_np, input_sinogram_np, full=True, data_range=1.0)\n",
    "    ssim_values.append(ssim_value)\n",
    "\n",
    "    # Load ground truth sinogram\n",
    "    gt_sinogram = Image.open(gt_path).convert('L')  \n",
    "    gt_sinogram = resize_transform(gt_sinogram)\n",
    "    gt_sinogram = ToTensor()(gt_sinogram)  \n",
    "\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(8, 3))\n",
    "    axs[0].imshow(input_sinogram_np, cmap='gray')\n",
    "    axs[0].set_title('Original')\n",
    "    axs[0].axis('off')\n",
    "    axs[1].imshow(denoised_output_np, cmap='gray')\n",
    "    axs[1].set_title('Predicted (Denoised Output)')\n",
    "    axs[1].axis('off')\n",
    "    axs[2].imshow(gt_sinogram.squeeze().numpy(), cmap='gray') \n",
    "    axs[2].set_title('Ground Truth')\n",
    "    axs[2].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i, input_path in enumerate(input_sinogram_paths):\n",
    "    print(f\"Input: {input_path}, PSNR: {psnr_values[i]:.4f}, SSIM: {ssim_values[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unet after hyperparameters were tuned and regularisation added as well as early stopping to prevent overfitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.metrics import mean_squared_error, structural_similarity\n",
    "from tensorflow.image import ssim\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers, regularizers\n",
    "\n",
    "class unet_sinogram(tf.keras.Model):\n",
    "    def __init__(self, width=128, depth=5, dropout_rate=0.1, l2_lambda=0.001):\n",
    "        super(unet_sinogram, self).__init__()\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_lambda = l2_lambda\n",
    "        self.history = {'loss': [], 'accuracy': [], 'val_loss': [], 'val_accuracy': []}\n",
    "\n",
    "        # U-Net model architecture for sinograms\n",
    "        self.inputs = tf.keras.Input(shape=(128, 128, 1))\n",
    "        x = self.inputs\n",
    "\n",
    "        # Encoder\n",
    "        for _ in range(depth):\n",
    "            x = layers.Conv2D(width, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = layers.Conv2D(width, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "\n",
    "        # Decoder\n",
    "        for _ in range(depth):\n",
    "            x = layers.Conv2DTranspose(width, 2, strides=(2, 2), padding='same')(x)\n",
    "            x = layers.Conv2D(width, 3, activation='relu', padding='same', kernel_regularizer=regularizers.l2(l2_lambda))(x)\n",
    "            x = layers.BatchNormalization()(x)\n",
    "            x = layers.Dropout(self.dropout_rate)(x)\n",
    "\n",
    "        self.outputs = layers.Conv2D(1, 1, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "        self.model = tf.keras.Model(inputs=self.inputs, outputs=self.outputs, name=f\"sinogram_unet_{width}_{depth}\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.model(inputs)\n",
    "\n",
    "    def compile_model(self, optimizer, loss_function, metrics):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss_function, metrics=metrics)\n",
    "\n",
    "    def train_model(self, train_input_images, train_output_images, epochs, batch_size, validation_split):\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)  # Define early stopping callback\n",
    "        history = self.model.fit(train_input_images, train_output_images, epochs=epochs, batch_size=batch_size, validation_split=validation_split, callbacks=[early_stopping], verbose=1)\n",
    "        self.history['loss'].extend(history.history['loss'])\n",
    "        self.history['accuracy'].extend(history.history['accuracy'])\n",
    "        self.history['val_loss'].extend(history.history['val_loss'])\n",
    "        self.history['val_accuracy'].extend(history.history['val_accuracy'])\n",
    "\n",
    "        self.save_metrics()\n",
    "\n",
    "    def evaluate_model(self, test_input_images, test_output_images):\n",
    "        return self.model.evaluate(test_input_images, test_output_images, verbose=0)\n",
    "\n",
    "    def save_metrics(self):\n",
    "        with open(\"training_metrics.txt\", \"w\") as file:\n",
    "            file.write(\"Epoch\\tLoss\\tAccuracy\\tVal_Loss\\tVal_Accuracy\\n\")\n",
    "            for i in range(len(self.history['loss'])):\n",
    "                file.write(f\"{i+1}\\t{self.history['loss'][i]}\\t{self.history['accuracy'][i]}\\t{self.history['val_loss'][i]}\\t{self.history['val_accuracy'][i]}\\n\")\n",
    "\n",
    "    def calculate_metrics(self, test_input_images, test_output_images):\n",
    "        mse_values = []\n",
    "        ssim_values = []\n",
    "        psnr_values = []\n",
    "\n",
    "        for i in range(len(test_input_images)):\n",
    "            input_image = test_input_images[i]\n",
    "            output_image = test_output_images[i]\n",
    "            predicted_image = self.model.predict(np.expand_dims(input_image, axis=0))[0]\n",
    "\n",
    "            mse = mean_squared_error(output_image, predicted_image)\n",
    "            ssim_score = structural_similarity(output_image, predicted_image, data_range=predicted_image.max() - predicted_image.min())\n",
    "            psnr = tf.image.psnr(output_image, predicted_image, max_val=1.0)\n",
    "\n",
    "            mse_values.append(mse)\n",
    "            ssim_values.append(ssim_score)\n",
    "            psnr_values.append(psnr)\n",
    "\n",
    "        return mse_values, ssim_values, psnr_values\n",
    "\n",
    "# custom loss functions\n",
    "def ssim_loss(y_true, y_pred):\n",
    "    return 1 - tf.reduce_mean(ssim(y_true, y_pred, max_val=1.0))\n",
    "\n",
    "def psnr_loss(y_true, y_pred):\n",
    "    return -tf.image.psnr(y_true, y_pred, max_val=1.0)\n",
    "\n",
    "def load_and_preprocess_sinograms(input_folder, output_folder):\n",
    "    input_images = []\n",
    "    output_images = []\n",
    "    \n",
    "    input_filenames = sorted(os.listdir(input_folder))\n",
    "    output_filenames = sorted(os.listdir(output_folder))\n",
    "    \n",
    "    for input_filename, output_filename in zip(input_filenames, output_filenames):\n",
    "        input_img_path = os.path.join(input_folder, input_filename)\n",
    "        output_img_path = os.path.join(output_folder, output_filename)\n",
    "        \n",
    "        input_img = cv2.imread(input_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        output_img = cv2.imread(output_img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        if input_img is not None and output_img is not None:\n",
    "            input_img = cv2.resize(input_img, (128, 128))  \n",
    "            output_img = cv2.resize(output_img, (128, 128)) \n",
    "            \n",
    "            input_img = input_img.astype('float32') / 255.0  \n",
    "            output_img = output_img.astype('float32') / 255.0 \n",
    "            \n",
    "            input_images.append(input_img)\n",
    "            output_images.append(output_img)\n",
    "    \n",
    "    return np.array(input_images), np.array(output_images)\n",
    "\n",
    "input_folder = '/jupyter/work/fyp/data/sinograms/3rd_set/4'\n",
    "output_folder = '/jupyter/work/fyp/data/sinograms/3rd_set/16'\n",
    "\n",
    "input_images, output_images = load_and_preprocess_sinograms(input_folder, output_folder)\n",
    "\n",
    "train_input_images, test_input_images, train_output_images, test_output_images = train_test_split(\n",
    "    input_images, output_images, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "unet_model = unet_sinogram()\n",
    "unet_model.compile_model(optimizer=Adam(learning_rate=0.00001), loss_function=psnr_loss, metrics=['accuracy'])\n",
    "\n",
    "unet_model.train_model(train_input_images, train_output_images, epochs=200, batch_size=32, validation_split=0.2)\n",
    "\n",
    "unet_model.model.save(\"unet_sinogram_model.h5\")\n",
    "\n",
    "# Evaluate the model and calculate metrics\n",
    "# mse_values, ssim_values, psnr_values = unet_model.calculate_metrics(test_input_images, test_output_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PSNR graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations\n",
    "\n",
    "test_loss, _ = unet_model.evaluate_model(test_input_images, test_output_images)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "# calculate PSNR\n",
    "psnr = -test_loss  # No need to subscript test_loss\n",
    "print(f\"PSNR: {psnr}\")\n",
    "\n",
    "# plot loss and PSNR over epochs\n",
    "plt.plot(unet_model.history['loss'], label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy and Loss graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "train_losses = unet_model.history['loss']\n",
    "val_losses = unet_model.history['val_loss']\n",
    "train_accs = unet_model.history['accuracy']  \n",
    "val_accs = unet_model.history['val_accuracy']  \n",
    "\n",
    "# Loss Curves\n",
    "def plot_loss_curves(train_losses, val_losses, task_name, save_dir):\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f'{task_name} Loss Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'{task_name}_loss_curves.png'))\n",
    "    plt.show()\n",
    "\n",
    "# # Accuracy Curves\n",
    "def plot_accuracy_curves(train_accs, val_accs, task_name, save_dir):\n",
    "    plt.plot(train_accs, label='Training Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title(f'{task_name} Accuracy Curves')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(save_dir, f'{task_name}_accuracy_curves.png'))\n",
    "    plt.show()\n",
    "\n",
    "results_folder = '/jupyter/work/fyp/code/results'\n",
    "\n",
    "plot_loss_curves(train_losses, val_losses, 'Image Reconstruction', results_folder)\n",
    "plot_accuracy_curves(train_accs, val_accs, 'Image Reconstruction', results_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test regularised model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import load_model\n",
    "from skimage.metrics import peak_signal_noise_ratio\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# PSNR loss function\n",
    "def psnr_loss(y_true, y_pred):\n",
    "    return -K.mean(10.0 * K.log(K.square(1.0) / (K.square(y_pred - y_true) + K.epsilon())) / K.log(10.0))\n",
    "\n",
    "# trained model\n",
    "model = load_model(\"unet_sinogram_model.h5\", custom_objects={'psnr_loss': psnr_loss})\n",
    "\n",
    "# Define function to load and preprocess a single image\n",
    "def load_and_preprocess_image(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    if image is not None:\n",
    "        image = cv2.resize(image, (128, 128))  \n",
    "        image = image.astype('float32') / 255.0 \n",
    "    return image\n",
    "\n",
    "input_folder = '/jupyter/work/fyp/data/sinograms/3rd_set/4'\n",
    "output_folder = '/jupyter/work/fyp/data/sinograms/3rd_set/16'\n",
    "\n",
    "image_filenames = os.listdir(input_folder)\n",
    "\n",
    "num_images_to_test = 20  # Adjust this number as needed\n",
    "random_image_filenames = random.sample(image_filenames, num_images_to_test)\n",
    "\n",
    "images = []\n",
    "\n",
    "for random_image_filename in random_image_filenames:\n",
    "    input_image_path = os.path.join(input_folder, random_image_filename)\n",
    "    input_image = load_and_preprocess_image(input_image_path)\n",
    "\n",
    "    # reshape input image for model prediction\n",
    "    input_image = np.expand_dims(input_image, axis=0)\n",
    "\n",
    "    # predict the output image using the trained model\n",
    "    output_image = model.predict(input_image)\n",
    "\n",
    "    ground_truth_image_path = os.path.join(output_folder, random_image_filename)\n",
    "    \n",
    "    try:\n",
    "        ground_truth_image = load_and_preprocess_image(ground_truth_image_path)\n",
    "\n",
    "        # PSNR (Peak Signal-to-Noise Ratio)\n",
    "        psnr_value = peak_signal_noise_ratio(ground_truth_image, output_image[0, :, :, 0])\n",
    "\n",
    "        # Plot original, predicted, and ground truth images\n",
    "        plt.figure(figsize=(10, 5))\n",
    "\n",
    "        # Original input image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(input_image[0], cmap='gray')\n",
    "        plt.title('Original Input Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Predicted output image\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(output_image[0, :, :, 0], cmap='gray')\n",
    "        plt.title('Predicted Output Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        # Ground truth output image\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(ground_truth_image, cmap='gray')\n",
    "        plt.title('Ground Truth Output Image')\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        # PSNR value\n",
    "        print(f\"PSNR for {random_image_filename}: {psnr_value}\")\n",
    "\n",
    "        images.append(np.concatenate([input_image[0], output_image[0, :, :, 0], ground_truth_image], axis=1))\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading or processing {ground_truth_image_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "grid_image = np.concatenate(images, axis=0)\n",
    "\n",
    "cv2.imwrite(\"grid_image.png\", grid_image)\n",
    "\n",
    "plt.imshow(grid_image, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a30072751ebdd9e38b3a3723274826dbc6c580cc83e8305b93d67478e6165946"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
